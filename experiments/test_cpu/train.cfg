[config]

mode = train
pretrained_model = Qwen/Qwen3-0.6B

# Tiny data for testing
train_num_sentences = 8
train_max_sentence_len = 32

# Small GRPO
grpo_group_size = 2
grpo_beta = 0.1
grpo_temperature = 0.7
reward_metric = chrf
alpha = 1.0

# Minimal training
max_epochs = 1
learning_rate = 1e-5
batch_size = 2
gradient_accumulation_steps = 1
warmup_steps = 1
eval_every_n_steps = 0
max_tokens = 64

# Only one language for speed
target_languages = yo

# Logging
wandb_project = rl-translation-cycles
wandb_run_name = test-cpu

models_dir = checkpoints/test_cpu
