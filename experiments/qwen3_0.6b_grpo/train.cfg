[config]

mode = train
pretrained_model = Qwen/Qwen3-0.6B
language = fra_Latn

# Data
train_dataset = HuggingFaceFW/fineweb
train_dataset_subset = sample-10BT
train_num_sentences = 20000
train_max_sentence_len = 128

# GRPO
grpo_group_size = 4
grpo_beta = 0.1
grpo_temperature = 0.7
grpo_top_p = 0.9
reward_metric = chrf
alpha = 1.0

# Training
max_epochs = 3
learning_rate = 1e-5
batch_size = 4
gradient_accumulation_steps = 4
grad_norm = 1.0
warmup_steps = 100
eval_every_n_steps = 500

# Model
max_tokens = 256

# Logging
wandb_project = rl-translation-cycles
wandb_run_name = qwen3-0.6b-grpo

# Checkpointing
models_dir = checkpoints/qwen3_0.6b_grpo
