[config]

mode = train
pretrained_model = Qwen/Qwen3-0.6B
language = fra_Latn

train_dataset = HuggingFaceFW/fineweb
train_dataset_subset = sample-10BT
train_num_sentences = 40000
train_max_sentence_len = 128

grpo_group_size = 4
grpo_beta = 0.1
grpo_temperature = 0.7
grpo_top_p = 0.9
reward_metric = chrf
alpha = 1.0

max_epochs = 3
learning_rate = 1e-5
batch_size = 6
gradient_accumulation_steps = 4

wandb_project = rl-translation-cycles
wandb_run_name = qwen3-0.6b-grpo

models_dir = checkpoints/qwen3_0.6b_grpo
