[config]

mode = train
pretrained_model = Qwen/Qwen3-0.6B
language = fra_Latn

train_dataset = HuggingFaceFW/fineweb
train_dataset_subset = sample-10BT
train_num_sentences = 128 
train_max_sentence_len = 128

grpo_group_size = 5 
grpo_beta = 0.1
grpo_temperature = 0.7
grpo_top_p = 0.9
reward_metric = bleu
alpha = 0.0

max_epochs = 5 
learning_rate = 1e-5
batch_size = 16 
gradient_accumulation_steps = 2

wandb_project = rl-translation-cycles
wandb_run_name = qwen3-0.6b-grpo

models_dir = checkpoints/qwen3_0.6b_grpo
